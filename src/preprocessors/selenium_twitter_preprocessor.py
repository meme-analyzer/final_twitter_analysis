import os
import pandas as pd
import numpy as np
import re
from datetime import datetime
import sys
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.config import RAW_DATA_DIR, PROCESSED_DATA_DIR

class SeleniumTwitterPreprocessor:
    def __init__(self):
        self.raw_data_dir = RAW_DATA_DIR
        self.processed_data_dir = PROCESSED_DATA_DIR
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')

    def load_twitter_data(self, filename):
        filepath = os.path.join(self.raw_data_dir, filename)
        df = pd.read_csv(filepath)
        print(f"ğŸ“¥ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ê²Œì‹œë¬¼")
        return df

    def preprocess(self, df):
        print("\nğŸ§¹=== íŠ¸ìœ„í„° ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")

        # ë‚ ì§œ, ì‹œê°„, ìš”ì¼ ì¶”ì¶œ
        df['created_at'] = pd.to_datetime(df['created_at'])
        df['date'] = df['created_at'].dt.date
        df['hour'] = df['created_at'].dt.hour
        df['day_of_week'] = df['created_at'].dt.dayofweek

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df['text'] = df['text'].fillna('')
        df['author'] = df['author'].fillna('[deleted]')
        df['likes'] = pd.to_numeric(df.get('likes', 0), errors='coerce').fillna(0).astype(int)
        df['retweets'] = pd.to_numeric(df.get('retweets', 0), errors='coerce').fillna(0).astype(int)
        df['views'] = pd.to_numeric(df.get('views', 0), errors='coerce').fillna(0).astype(int)

        # í…ìŠ¤íŠ¸ í´ë Œì§•
        df['text_clean'] = df['text'].apply(self.clean_text)

        # ì°¸ì—¬ ì ìˆ˜ ê³„ì‚°
        df['engagement_score'] = df['likes'] + df['retweets'] * 2 + df['views'] * 0.1

        # ìµœì‹ ìˆœ ì •ë ¬
        df = df.sort_values(by='created_at', ascending=False).reset_index(drop=True)

        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ê²Œì‹œë¬¼")
        return df

    def clean_text(self, text):
        if pd.isna(text) or text == '':
            return ''
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def analyze_temporal_patterns(self, df):
        print("\nâ±ï¸=== ì‹œê°„ íŒ¨í„´ ë¶„ì„ ===")
        daily_posts = df.groupby('date').size()
        hourly_dist = df['hour'].value_counts().sort_index()
        day_dist = df['day_of_week'].value_counts().sort_index()
        days = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']

        print(f"ğŸ“† ë°ì´í„° ê¸°ê°„: {daily_posts.index.min()} ~ {daily_posts.index.max()}")
        print(f"ğŸ“ˆ ì¼ í‰ê·  ê²Œì‹œë¬¼ ìˆ˜: {daily_posts.mean():.2f}")
        print(f"â° ê°€ì¥ í™œë°œí•œ ì‹œê°„ëŒ€: {hourly_dist.idxmax()}ì‹œ")
        print(f"ğŸ—“ï¸ ê°€ì¥ í™œë°œí•œ ìš”ì¼: {days[day_dist.idxmax()]}ìš”ì¼")

        return {
            'daily_posts': daily_posts,
            'hourly_dist': hourly_dist,
            'day_dist': day_dist
        }

    def perform_clustering(self, df, n_clusters=5):
        print("\nğŸ”—=== í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ ===")
        embeddings = self.embedder.encode(df['text_clean'].tolist(), show_progress_bar=True)
        pca = PCA(n_components=2)
        reduced = pca.fit_transform(embeddings)
        df['x'] = reduced[:, 0]
        df['y'] = reduced[:, 1]
        kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(reduced)
        df['cluster'] = kmeans.labels_
        print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ (êµ°ì§‘ ìˆ˜: {n_clusters})")
        return df

    def estimate_last_seen(self, df):
        print("\nğŸ”=== ìƒì¡´ ë¶„ì„ìš© ë§ˆì§€ë§‰ ë“±ì¥ ì‹œì  ì¶”ì • ===")
        grouped = df.groupby('text_clean')['created_at'].max().reset_index()
        grouped.columns = ['text_clean', 'last_seen_at']
        df = df.merge(grouped, on='text_clean', how='left')
        print("âœ… last_seen_at ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ")
        return df

    def save_processed_data(self, df, output_filename):
        os.makedirs(self.processed_data_dir, exist_ok=True)
        output_path = os.path.join(self.processed_data_dir, output_filename)
        df.to_csv(output_path, index=False)
        print(f"ğŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥: {output_path}")

        summary = {
            'total_posts': len(df),
            'date_range': f"{df['created_at'].min()} ~ {df['created_at'].max()}",
            'unique_authors': df['author'].nunique(),
            'avg_likes': df['likes'].mean(),
            'avg_retweets': df['retweets'].mean(),
            'avg_views': df['views'].mean()
        }

        summary_path = output_path.replace('.csv', '_summary.txt')
        with open(summary_path, 'w', encoding='utf-8') as f:
            for key, value in summary.items():
                f.write(f"{key}: {value}\n")

        return df
